
[{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/a/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"A","type":"people"},{"content":"","date":"1 January 0001","externalUrl":"https://zl3466.github.io","permalink":"/people/zhiheng-li/","section":"Current Members","summary":"Zhiheng is an undergraduate student from Tandon Electrical and Computer Engineering major. His research and career interests lie in the fields of medical engineering, autonomous vehicle, and computer vision.","title":"Zhiheng Li","type":"people"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/g/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"A","type":"people"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/b/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"B","type":"people"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/c/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"C","type":"people"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/d/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"D","type":"people"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/e/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"E","type":"people"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/people/f/","section":"Current Members","summary":"Yiming is a PhD student. He received his B.E. in Tongji University. His research interest is Robot Perception, Navigation and Machine Learning.","title":"F","type":"people"},{"content":"To be updated\n","date":"14 August 2020","externalUrl":null,"permalink":"/benchmarks/","section":"Benchmarks","summary":"To be updated","title":"Benchmarks","type":"benchmarks"},{"content":"Some treasured life experiences, such as the first step onto college campus or the first glimpse of a loved one, can remain etched in our memories for a lifetime. Where and how different memories – an episode of life experience or a concept about a person – reside in the brain remains a mystery. We are intrigued by long-standing questions such as:\nHow do neural networks encode and store episodic and semantic information? How is stored information retrieved and utilized to support cognitive processes like planning and thinking? What molecules underpin memory processes, especially memory storage? If these mechanisms are deciphered, we might be able to read and manipulate memories, such as enhancing memory and cognition in Alzheimer's disease and erasing traumatic memories associated with PTSD.\n","date":"14 August 2020","externalUrl":null,"permalink":"/","section":"CHEN LAB","summary":"Some treasured life experiences, such as the first step onto college campus or the first glimpse of a loved one, can remain etched in our memories for a lifetime. Where and how different memories – an episode of life experience or a concept about a person – reside in the brain remains a mystery.","title":"CHEN LAB","type":"page"},{"content":"","date":"14 August 2020","externalUrl":null,"permalink":"/tags/sample/","section":"Tags","summary":"","title":"sample","type":"tags"},{"content":"","date":"14 August 2020","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"14 August 2020","externalUrl":null,"permalink":"/tags/team/","section":"Tags","summary":"","title":"team","type":"tags"},{"content":"The MARS dataset is curated by members of the AI4CE Lab from New York University. The work is made possible by May Mobility with its brilliant team and fleets of autonomous vehicles.\n","date":"14 August 2020","externalUrl":null,"permalink":"/team/","section":"Team","summary":"The MARS dataset is curated by members of the AI4CE Lab from New York University. The work is made possible by May Mobility with its brilliant team and fleets of autonomous vehicles.","title":"Team","type":"team"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"This section contains all my current projects.\n","date":"1 January 0001","externalUrl":null,"permalink":"/people/","section":"Current Members","summary":"This section contains all my current projects.","title":"Current Members","type":"people"},{"content":" Vehicle Setup #\rSensor Specifications #\rSensor Details 1 \\(\\times\\) LiDAR 10Hz, 128 channel, horizontal FoV \\(360^\\circ\\), vertical FoV \\(40^\\circ\\) 3 \\(\\times\\) RGB Camrea 10Hz, original resolution \\(1440 \\times 928\\), sampled to \\(720 \\times 464\\), Horizontal FoV \\(60^\\circ \\), Vertical FoV \\(40^\\circ\\) 3 \\(\\times\\) Fisheye Camrea 10Hz, original resolution \\(1240 \\times 728\\), sampled to \\(620 \\times 364\\), horizontal FoV \\(140^\\circ\\), vertical FoV \\(88^\\circ\\) 1 \\(\\times\\) IMU 10Hz, velocity, angular velocity, acceleration 1 \\(\\times\\) GPS 10Hz, longitude, latitude, elevation Dataset #\rWe curate two subsets in MARS: one multiagent, which facilitates collaborative driving with multiple vehicles simultaneously present at the same location, and one multitraversal, which enables memory retrospection through asynchronous traversals of the same location by multiple vehicles.\n","date":"1 January 0001","externalUrl":null,"permalink":"/data/","section":"Dataset","summary":"Vehicle Setup #\rSensor Specifications #\rSensor Details 1 \\(\\times\\) LiDAR 10Hz, 128 channel, horizontal FoV \\(360^\\circ\\), vertical FoV \\(40^\\circ\\) 3 \\(\\times\\) RGB Camrea 10Hz, original resolution \\(1440 \\times 928\\), sampled to \\(720 \\times 464\\), Horizontal FoV \\(60^\\circ \\), Vertical FoV \\(40^\\circ\\) 3 \\(\\times\\) Fisheye Camrea 10Hz, original resolution \\(1240 \\times 728\\), sampled to \\(620 \\times 364\\), horizontal FoV \\(140^\\circ\\), vertical FoV \\(88^\\circ\\) 1 \\(\\times\\) IMU 10Hz, velocity, angular velocity, acceleration 1 \\(\\times\\) GPS 10Hz, longitude, latitude, elevation Dataset #\rWe curate two subsets in MARS: one multiagent, which facilitates collaborative driving with multiple vehicles simultaneously present at the same location, and one multitraversal, which enables memory retrospection through asynchronous traversals of the same location by multiple vehicles.","title":"Dataset","type":"data"},{"content":" To be updated with download link #\r3-month span, 67 locations, 1.4 million frames.\nAmong the 67 locations, 48 have over 20 traversals, 23 over 100 traversals, and 6 over 200 traversals. Each traversal has 250 frames (25 seconds) on average, with the majority of traversals containing 100 to 400 frames (10 to 40 seconds).\nDemo Videos #\rScene 1, \u0026ldquo;T-way\u0026rdquo; encounter #\rThere should have been a video here but your browser does not seem\rto support it.\rScene 2, encounter across a vegetation #\rThere should have been a video here but your browser does not seem\rto support it.\rScene 18, straight road tailgating #\rThere should have been a video here but your browser does not seem\rto support it.\rScene 21, three agents at intersection #\rThere should have been a video here but your browser does not seem\rto support it.\rScene 29, tailgating at corner #\rThere should have been a video here but your browser does not seem\rto support it.\rScene 30, opposite way encounter #\rThere should have been a video here but your browser does not seem\rto support it.\r","date":"1 January 0001","externalUrl":null,"permalink":"/data/multiagent/","section":"Dataset","summary":"To be updated with download link #\r3-month span, 67 locations, 1.4 million frames.\nAmong the 67 locations, 48 have over 20 traversals, 23 over 100 traversals, and 6 over 200 traversals.","title":"Multiagent","type":"data"},{"content":" To be updated with download link #\r3-month span, 67 locations, 1.4 million frames.\nAmong the 67 locations, 48 have over 20 traversals, 23 over 100 traversals, and 6 over 200 traversals. Each traversal has 250 frames (25 seconds) on average, with the majority of traversals containing 100 to 400 frames (10 to 40 seconds).\nDemo Videos #\rLocation 3, narrow road #\rThere should have been a video here but your browser does not seem\rto support it.\rLocation 15, intersection #\rThere should have been a video here but your browser does not seem\rto support it.\rLocation 24 intersection, multiple direction #\rThere should have been a video here but your browser does not seem\rto support it.\rLocation 57, open field #\rThere should have been a video here but your browser does not seem\rto support it.\rLocation 61, highway \u0026amp; bridge #\rThere should have been a video here but your browser does not seem\rto support it.\r","date":"1 January 0001","externalUrl":null,"permalink":"/data/multitraversal/","section":"Dataset","summary":"To be updated with download link #\r3-month span, 67 locations, 1.4 million frames.\nAmong the 67 locations, 48 have over 20 traversals, 23 over 100 traversals, and 6 over 200 traversals.","title":"Multitraversal","type":"data"},{"content":"Our lab’s research focuses on elucidating how memory is encoded, retained, and retrieved within neural networks and molecules in the brain. We employ and develop cutting-edge tools to record and manipulate neuronal and molecular activities to unravel these mysteries. Our approach is integrative, expanding from electrophysiology, imaging, behavior, genetic manipulations, omics, to computational modeling.\nElucidating neural networks supporting memory encoding and consolidation #\rMuch of our work focuses on the hippocampus, a brain region giving rise to neural sequences underlying episodic memory. These sequences, occurring during either behavior (theta sequences) or sleep (replay sequences), make the foundation of memory encoding and consolidation. We believe what drives these sequences is a brain-wide oscillation network, and our lab is elucidating the key nodes in this network and how information is transmitted among them. One recent focus of the lab is an understudied hypothalamic structure in the brain’s oscillation network, the supramammillary nucleus. Unique about this small subcortical nucleus is its broad, extensive connections to the hippocampus and cortex. We are investigating how these long-range bottom-up pathways modulate network dynamics to process memory and cognition. See how we identified the supramammillary nucleus as a hypothalamic novelty hub and how it modulates different types of memories in the hippocampus.\nDeciphering neural computation underlying memory-guided behavior #\rComplex cognitive activities, such as planning and thinking, rely on established knowledge about the external world. We believe that the design of elaborate behavioral tasks, combined with large-scale in vivo recordings of neural activity, can lead to new discoveries about the underlying neural computations. Our lab employs this approach to investigate how the brain synthesizes novel ideas and solutions from stored information – i.e. memory retrieval and utilization. By drawing inspiration from psychology, engineering, and computer sciences, we aim to dissect neural mechanisms underlying memory-guided complex behaviors.\nDeveloping next-generation noninvasive brain machine interfaces for memory and cognitive control #\rManipulating memory and cognition requires technologies to spatiotemporally modulate nervous systems in deep tissue, sometimes in a precision closed-loop manner. For decades, physicians have utilized implanted electrodes connected through wires to a pacemaker-like device under the skin of the chest to electrically stimulate neurons deep in the brain. This approach, known as deep brain stimulation, has proved to be effective, but is costly, highly invasive, and lacks cell specificity. Our lab is developing noninvasive, high-precision neuromodulation technologies to manipulate diverse deep structures in the nervous system that are conventionally hard to reach. We adopt a cross-disciplinary approach by designing and synthesizing molecularly engineered, noninvasively delivered nano- or micro-interfaces capable of converting tissue-penetrable stimuli into signals sensible by genetically defined brain cells. We believe such efforts will contribute to the field’s collective efforts in unlocking diverse routes towards next-generation, noninvasive neuromodulation therapies. See how we developed and showcased a minimally invasive technology, near-infrared upconversion optogenetics, for various neuromodulation applications, e.g. recalling a stored memory by transcranial near-infrared light.\nLinking molecular and network pathologies of Alzheimer’s disease #\rAlzheimer\u0026rsquo;s disease (AD) is the most prevalent cause of dementia in the elderly. There is currently no cure for AD, and clinical trials only provide modest benefits. While characterized by pathological molecules like amyloid-β, tau and APOE4, AD is a mental and cognitive disorder. How the molecular pathologies lead to cognitive decline remains elusive. Bridging this gap requires understanding neural circuit mechanisms. By taking advantage of our expertise in systems neuroscience, our lab is identifying circuit pathology and developing circuit-targeting therapies for AD. See how we demonstrated the efficacy of a peptide drug that rescues cognitive decline and synaptic function in an animal model of advanced-stage AD.\n","date":"1 January 0001","externalUrl":null,"permalink":"/research/","section":"Research","summary":"Our lab’s research focuses on elucidating how memory is encoded, retained, and retrieved within neural networks and molecules in the brain. We employ and develop cutting-edge tools to record and manipulate neuronal and molecular activities to unravel these mysteries.","title":"Research","type":"research"},{"content":"","date":"1 January 0001","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]